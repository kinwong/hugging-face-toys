{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch HuggingFace GPU acceleration tests\n",
    "<https://towardsdatascience.com/gpu-acceleration-comes-to-pytorch-on-m1-macs-195c399efcc1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA/MPS is available\n",
    "\n",
    "import platform\n",
    "import torch\n",
    "\n",
    "if platform.platform().startswith('macOS'):\n",
    "    if torch.has_mps:\n",
    "        print('MPS is available')\n",
    "    else:\n",
    "        print('MPS is not available')\n",
    "        exit(1)\n",
    "else:\n",
    "    if platform.platform().startswith('win'):\n",
    "        if torch.has_cuda:\n",
    "            print('CUDA is available')\n",
    "        else:\n",
    "            print('CUDA is not available')\n",
    "            exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 5.09k/5.09k [00:00<00:00, 1.87MB/s]\n",
      "Downloading metadata: 100%|██████████| 3.34k/3.34k [00:00<00:00, 2.07MB/s]\n",
      "Downloading readme: 100%|██████████| 10.6k/10.6k [00:00<00:00, 3.08MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset trec/default to /Users/kinwong/.cache/huggingface/datasets/trec/default/2.0.0/f2469cab1b5fceec7249fda55360dfdbd92a7a5b545e91ea0f78ad108ffac1c2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 336k/336k [00:01<00:00, 234kB/s]\n",
      "Downloading data: 100%|██████████| 23.4k/23.4k [00:00<00:00, 96.7kB/s]\n",
      "Downloading data files: 100%|██████████| 2/2 [00:06<00:00,  3.43s/it]\n",
      "                                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset trec downloaded and prepared to /Users/kinwong/.cache/huggingface/datasets/trec/default/2.0.0/f2469cab1b5fceec7249fda55360dfdbd92a7a5b545e91ea0f78ad108ffac1c2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'coarse_label', 'fine_label'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset  # pip install datasets\n",
    "\n",
    "# load the first 1K rows of the TREC dataset\n",
    "trec = load_dataset('trec', split='train[:1000]')\n",
    "trec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'How did serfdom develop in and then leave Russia ?',\n",
       " 'coarse_label': 2,\n",
       " 'fine_label': 26}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 11.1kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 240kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:01<00:00, 225kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:01<00:00, 363kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 440M/440M [01:06<00:00, 6.61MB/s] \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel  # pip install transformers\n",
    "\n",
    "# initialize BERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# take the first 64 rows of the trec data\n",
    "text = trec['text'][:64]\n",
    "\n",
    "# tokenize text using the BERT tokenizer\n",
    "tokens = tokenizer(\n",
    "    text, max_length=512,\n",
    "    truncation=True, padding=True,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Tests\n",
    "When processing these tokens on CPU we get an average processing time of 547ms. We can switch this to MPS by moving the tokens tensor and model to the MPS device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('mps')\n",
    "model.to(device)\n",
    "tokens.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244 ms ± 1.84 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "model(**tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt-hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "adf58c87b26d4d4eadfb583e93675b75323ab4346510f86229823dd08facd68d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
