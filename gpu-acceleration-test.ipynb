{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch HuggingFace GPU acceleration tests\n",
    "<https://towardsdatascience.com/gpu-acceleration-comes-to-pytorch-on-m1-macs-195c399efcc1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Kinha\\.conda\\envs\\pt-hf\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "platform.platform = Windows-10-10.0.22621-SP0\n",
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA/MPS is available\n",
    "\n",
    "import platform\n",
    "import torch\n",
    "\n",
    "print(f'platform.platform = {platform.platform()}')\n",
    "if platform.platform().startswith('macOS'):\n",
    "    if torch.has_mps:\n",
    "        print('MPS is available')\n",
    "    else:\n",
    "        print('MPS is not available')\n",
    "        exit(1)\n",
    "else:\n",
    "    if platform.platform().startswith('Windows'):\n",
    "        if torch.has_cuda:\n",
    "            print('CUDA is available')\n",
    "        else:\n",
    "            print('CUDA is not available')\n",
    "            exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 5.09k/5.09k [00:00<00:00, 1.70MB/s]\n",
      "Downloading metadata: 100%|██████████| 3.34k/3.34k [00:00<00:00, 1.67MB/s]\n",
      "Downloading readme: 100%|██████████| 10.6k/10.6k [00:00<00:00, 3.55MB/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset  \u001b[39m# pip install datasets\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# load the first 1K rows of the TREC dataset\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m trec \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39;49m\u001b[39mtrec\u001b[39;49m\u001b[39m'\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain[:1000]\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m trec\n",
      "File \u001b[1;32md:\\Users\\Kinha\\.conda\\envs\\pt-hf\\lib\\site-packages\\datasets\\load.py:1719\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[0;32m   1716\u001b[0m ignore_verifications \u001b[39m=\u001b[39m ignore_verifications \u001b[39mor\u001b[39;00m save_infos\n\u001b[0;32m   1718\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 1719\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   1720\u001b[0m     path\u001b[39m=\u001b[39mpath,\n\u001b[0;32m   1721\u001b[0m     name\u001b[39m=\u001b[39mname,\n\u001b[0;32m   1722\u001b[0m     data_dir\u001b[39m=\u001b[39mdata_dir,\n\u001b[0;32m   1723\u001b[0m     data_files\u001b[39m=\u001b[39mdata_files,\n\u001b[0;32m   1724\u001b[0m     cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   1725\u001b[0m     features\u001b[39m=\u001b[39mfeatures,\n\u001b[0;32m   1726\u001b[0m     download_config\u001b[39m=\u001b[39mdownload_config,\n\u001b[0;32m   1727\u001b[0m     download_mode\u001b[39m=\u001b[39mdownload_mode,\n\u001b[0;32m   1728\u001b[0m     revision\u001b[39m=\u001b[39mrevision,\n\u001b[0;32m   1729\u001b[0m     use_auth_token\u001b[39m=\u001b[39muse_auth_token,\n\u001b[0;32m   1730\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   1731\u001b[0m )\n\u001b[0;32m   1733\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   1734\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32md:\\Users\\Kinha\\.conda\\envs\\pt-hf\\lib\\site-packages\\datasets\\load.py:1523\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_msg)\n\u001b[0;32m   1522\u001b[0m \u001b[39m# Instantiate the dataset builder\u001b[39;00m\n\u001b[1;32m-> 1523\u001b[0m builder_instance: DatasetBuilder \u001b[39m=\u001b[39m builder_cls(\n\u001b[0;32m   1524\u001b[0m     cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   1525\u001b[0m     config_name\u001b[39m=\u001b[39mconfig_name,\n\u001b[0;32m   1526\u001b[0m     data_dir\u001b[39m=\u001b[39mdata_dir,\n\u001b[0;32m   1527\u001b[0m     data_files\u001b[39m=\u001b[39mdata_files,\n\u001b[0;32m   1528\u001b[0m     \u001b[39mhash\u001b[39m\u001b[39m=\u001b[39m\u001b[39mhash\u001b[39m,\n\u001b[0;32m   1529\u001b[0m     features\u001b[39m=\u001b[39mfeatures,\n\u001b[0;32m   1530\u001b[0m     use_auth_token\u001b[39m=\u001b[39muse_auth_token,\n\u001b[0;32m   1531\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbuilder_kwargs,\n\u001b[0;32m   1532\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   1533\u001b[0m )\n\u001b[0;32m   1535\u001b[0m \u001b[39mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[1;32md:\\Users\\Kinha\\.conda\\envs\\pt-hf\\lib\\site-packages\\datasets\\builder.py:1292\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder.__init__\u001b[1;34m(self, writer_batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, writer_batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 1292\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1293\u001b[0m     \u001b[39m# Batch size used by the ArrowWriter\u001b[39;00m\n\u001b[0;32m   1294\u001b[0m     \u001b[39m# It defines the number of samples that are kept in memory before writing them\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m     \u001b[39m# and also the length of the arrow chunks\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m     \u001b[39m# None means that the ArrowWriter will use its default value\u001b[39;00m\n\u001b[0;32m   1297\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_writer_batch_size \u001b[39m=\u001b[39m writer_batch_size \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDEFAULT_WRITER_BATCH_SIZE\n",
      "File \u001b[1;32md:\\Users\\Kinha\\.conda\\envs\\pt-hf\\lib\\site-packages\\datasets\\builder.py:312\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[1;34m(self, cache_dir, config_name, hash, base_path, info, features, use_auth_token, repo_id, data_files, data_dir, name, **config_kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[39m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[39m# Prefill datasetinfo\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[39mif\u001b[39;00m info \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m     info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_exported_dataset_info()\n\u001b[0;32m    313\u001b[0m     info\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info())\n\u001b[0;32m    314\u001b[0m     info\u001b[39m.\u001b[39mbuilder_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\n",
      "File \u001b[1;32md:\\Users\\Kinha\\.conda\\envs\\pt-hf\\lib\\site-packages\\datasets\\builder.py:412\u001b[0m, in \u001b[0;36mDatasetBuilder.get_exported_dataset_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_exported_dataset_info\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DatasetInfo:\n\u001b[0;32m    401\u001b[0m     \u001b[39m\"\"\"Empty DatasetInfo if doesn't exist\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \n\u001b[0;32m    403\u001b[0m \u001b[39m    Example:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[39m    ```\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 412\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_all_exported_dataset_infos()\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mname, DatasetInfo())\n",
      "File \u001b[1;32md:\\Users\\Kinha\\.conda\\envs\\pt-hf\\lib\\site-packages\\datasets\\builder.py:398\u001b[0m, in \u001b[0;36mDatasetBuilder.get_all_exported_dataset_infos\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_all_exported_dataset_infos\u001b[39m(\u001b[39mcls\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DatasetInfosDict:\n\u001b[0;32m    387\u001b[0m     \u001b[39m\"\"\"Empty dict if doesn't exist\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \n\u001b[0;32m    389\u001b[0m \u001b[39m    Example:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[39m    ```\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[39mreturn\u001b[39;00m DatasetInfosDict\u001b[39m.\u001b[39;49mfrom_directory(\u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget_imported_module_dir())\n",
      "File \u001b[1;32md:\\Users\\Kinha\\.conda\\envs\\pt-hf\\lib\\site-packages\\datasets\\info.py:370\u001b[0m, in \u001b[0;36mDatasetInfosDict.from_directory\u001b[1;34m(cls, dataset_infos_dir)\u001b[0m\n\u001b[0;32m    368\u001b[0m     dataset_metadata \u001b[39m=\u001b[39m DatasetMetadata\u001b[39m.\u001b[39mfrom_readme(Path(dataset_infos_dir) \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mREADME.md\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    369\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdataset_info\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m dataset_metadata:\n\u001b[1;32m--> 370\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_metadata(dataset_metadata)\n\u001b[0;32m    371\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dataset_infos_dir, config\u001b[39m.\u001b[39mDATASETDICT_INFOS_FILENAME)):\n\u001b[0;32m    372\u001b[0m     \u001b[39m# this is just to have backward compatibility with dataset_infos.json files\u001b[39;00m\n\u001b[0;32m    373\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dataset_infos_dir, config\u001b[39m.\u001b[39mDATASETDICT_INFOS_FILENAME), encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[1;32md:\\Users\\Kinha\\.conda\\envs\\pt-hf\\lib\\site-packages\\datasets\\info.py:396\u001b[0m, in \u001b[0;36mDatasetInfosDict.from_metadata\u001b[1;34m(cls, dataset_metadata)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\n\u001b[0;32m    388\u001b[0m         {\n\u001b[0;32m    389\u001b[0m             dataset_info_yaml_dict\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mconfig_name\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m): DatasetInfo\u001b[39m.\u001b[39m_from_yaml_dict(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         }\n\u001b[0;32m    394\u001b[0m     )\n\u001b[0;32m    395\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 396\u001b[0m     dataset_info \u001b[39m=\u001b[39m DatasetInfo\u001b[39m.\u001b[39;49m_from_yaml_dict(dataset_metadata[\u001b[39m\"\u001b[39;49m\u001b[39mdataset_info\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    397\u001b[0m     dataset_info\u001b[39m.\u001b[39mconfig_name \u001b[39m=\u001b[39m dataset_metadata[\u001b[39m\"\u001b[39m\u001b[39mdataset_info\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mconfig_name\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    398\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m({dataset_info\u001b[39m.\u001b[39mconfig_name: dataset_info})\n",
      "File \u001b[1;32md:\\Users\\Kinha\\.conda\\envs\\pt-hf\\lib\\site-packages\\datasets\\info.py:332\u001b[0m, in \u001b[0;36mDatasetInfo._from_yaml_dict\u001b[1;34m(cls, yaml_data)\u001b[0m\n\u001b[0;32m    330\u001b[0m yaml_data \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(yaml_data)\n\u001b[0;32m    331\u001b[0m \u001b[39mif\u001b[39;00m yaml_data\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 332\u001b[0m     yaml_data[\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m Features\u001b[39m.\u001b[39;49m_from_yaml_list(yaml_data[\u001b[39m\"\u001b[39;49m\u001b[39mfeatures\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    333\u001b[0m \u001b[39mif\u001b[39;00m yaml_data\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39msplits\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    334\u001b[0m     yaml_data[\u001b[39m\"\u001b[39m\u001b[39msplits\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m SplitDict\u001b[39m.\u001b[39m_from_yaml_list(yaml_data[\u001b[39m\"\u001b[39m\u001b[39msplits\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32md:\\Users\\Kinha\\.conda\\envs\\pt-hf\\lib\\site-packages\\datasets\\features\\features.py:1745\u001b[0m, in \u001b[0;36mFeatures._from_yaml_list\u001b[1;34m(cls, yaml_data)\u001b[0m\n\u001b[0;32m   1742\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a dict or a list but got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(obj)\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mobj\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1745\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_dict(from_yaml_inner(yaml_data))\n",
      "File \u001b[1;32md:\\Users\\Kinha\\.conda\\envs\\pt-hf\\lib\\site-packages\\datasets\\features\\features.py:1741\u001b[0m, in \u001b[0;36mFeatures._from_yaml_list.<locals>.from_yaml_inner\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   1739\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, \u001b[39mlist\u001b[39m):\n\u001b[0;32m   1740\u001b[0m     names \u001b[39m=\u001b[39m [_feature\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m _feature \u001b[39min\u001b[39;00m obj]\n\u001b[1;32m-> 1741\u001b[0m     \u001b[39mreturn\u001b[39;00m {name: from_yaml_inner(_feature) \u001b[39mfor\u001b[39;00m name, _feature \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(names, obj)}\n\u001b[0;32m   1742\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a dict or a list but got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(obj)\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mobj\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Users\\Kinha\\.conda\\envs\\pt-hf\\lib\\site-packages\\datasets\\features\\features.py:1741\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1739\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, \u001b[39mlist\u001b[39m):\n\u001b[0;32m   1740\u001b[0m     names \u001b[39m=\u001b[39m [_feature\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m _feature \u001b[39min\u001b[39;00m obj]\n\u001b[1;32m-> 1741\u001b[0m     \u001b[39mreturn\u001b[39;00m {name: from_yaml_inner(_feature) \u001b[39mfor\u001b[39;00m name, _feature \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(names, obj)}\n\u001b[0;32m   1742\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a dict or a list but got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(obj)\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mobj\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Users\\Kinha\\.conda\\envs\\pt-hf\\lib\\site-packages\\datasets\\features\\features.py:1736\u001b[0m, in \u001b[0;36mFeatures._from_yaml_list.<locals>.from_yaml_inner\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   1734\u001b[0m             \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39m_type\u001b[39m\u001b[39m\"\u001b[39m: snakecase_to_camelcase(obj[\u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m])}\n\u001b[0;32m   1735\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m         \u001b[39mreturn\u001b[39;00m from_yaml_inner(obj[\u001b[39m\"\u001b[39;49m\u001b[39mdtype\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m   1737\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1738\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39m_type\u001b[39m\u001b[39m\"\u001b[39m: snakecase_to_camelcase(_type), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munsimplify(obj)[_type]}\n",
      "File \u001b[1;32md:\\Users\\Kinha\\.conda\\envs\\pt-hf\\lib\\site-packages\\datasets\\features\\features.py:1738\u001b[0m, in \u001b[0;36mFeatures._from_yaml_list.<locals>.from_yaml_inner\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   1736\u001b[0m             \u001b[39mreturn\u001b[39;00m from_yaml_inner(obj[\u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m   1737\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1738\u001b[0m         \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39m_type\u001b[39m\u001b[39m\"\u001b[39m: snakecase_to_camelcase(_type), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munsimplify(obj)[_type]}\n\u001b[0;32m   1739\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, \u001b[39mlist\u001b[39m):\n\u001b[0;32m   1740\u001b[0m     names \u001b[39m=\u001b[39m [_feature\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m _feature \u001b[39min\u001b[39;00m obj]\n",
      "File \u001b[1;32md:\\Users\\Kinha\\.conda\\envs\\pt-hf\\lib\\site-packages\\datasets\\features\\features.py:1706\u001b[0m, in \u001b[0;36mFeatures._from_yaml_list.<locals>.unsimplify\u001b[1;34m(feature)\u001b[0m\n\u001b[0;32m   1704\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(feature\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mclass_label\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(feature[\u001b[39m\"\u001b[39m\u001b[39mclass_label\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39mdict\u001b[39m):\n\u001b[0;32m   1705\u001b[0m     label_ids \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(feature[\u001b[39m\"\u001b[39m\u001b[39mclass_label\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m-> 1706\u001b[0m     \u001b[39mif\u001b[39;00m label_ids \u001b[39mand\u001b[39;00m label_ids \u001b[39m!=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(label_ids[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)):\n\u001b[0;32m   1707\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1708\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mClassLabel expected a value for all label ids [0:\u001b[39m\u001b[39m{\u001b[39;00mlabel_ids[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m] but some ids are missing.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1709\u001b[0m         )\n\u001b[0;32m   1710\u001b[0m     feature[\u001b[39m\"\u001b[39m\u001b[39mclass_label\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m [feature[\u001b[39m\"\u001b[39m\u001b[39mclass_label\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m][label_id] \u001b[39mfor\u001b[39;00m label_id \u001b[39min\u001b[39;00m label_ids]\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset  # pip install datasets\n",
    "\n",
    "# load the first 1K rows of the TREC dataset\n",
    "trec = load_dataset('trec', split='train[:1000]')\n",
    "trec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'How did serfdom develop in and then leave Russia ?',\n",
       " 'coarse_label': 2,\n",
       " 'fine_label': 26}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 11.1kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 240kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:01<00:00, 225kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:01<00:00, 363kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 440M/440M [01:06<00:00, 6.61MB/s] \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel  # pip install transformers\n",
    "\n",
    "# initialize BERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# take the first 64 rows of the trec data\n",
    "text = trec['text'][:64]\n",
    "\n",
    "# tokenize text using the BERT tokenizer\n",
    "tokens = tokenizer(\n",
    "    text, max_length=512,\n",
    "    truncation=True, padding=True,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Tests\n",
    "When processing these tokens on CPU we get an average processing time of 547ms. We can switch this to MPS by moving the tokens tensor and model to the MPS device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('mps')\n",
    "model.to(device)\n",
    "tokens.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244 ms ± 1.84 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "model(**tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt-hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c52a9d62af153a8eef4f36feb8d7785bbf534298b7b5fb0472260575b051be28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
